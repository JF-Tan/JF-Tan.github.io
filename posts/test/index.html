<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.68.3" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><meta name="description" content="Maximal Likehood Estimate (MLE) Here we change to another topic, the probability field. We know a lot of prob. distributions, like gassuan, possion, etc.. Now the setting is: you got plenty data samples, and you tell me what is the dist. of these samples thus I can use the dist. to predict other unknow samples.
So, now you have the data set:
$$ D = [ x_1, x_2, x_3, \dots, x_N ]$$"><title>Max Likehood Estimate(MLE)&nbsp;&ndash;&nbsp;Junfu&#39;s Blog</title><link rel="stylesheet" href="/css/core.min.0e2a5b265b3e3a731c53f1c959c008ba6aa6e8b7816db916127b3515fcc1a95143239a013afb161ae7f9ee498eaa5c0e.css" integrity="sha384-DipbJls&#43;OnMcU/HJWcAIumqm6LeBbbkWEns1FfzBqVFDI5oBOvsWGuf57kmOqlwO"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Max Likehood Estimate(MLE)" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name">Junfu's Blog</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/categories/">Categories</a><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/about">About</a><a class="nav item" href="https://google%2ecom"target="_blank" rel="noopener noreferrer">Google</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">Max Likehood Estimate(MLE)</h1><p class="article date">Monday, June 16, 2025</p></section><article class="article markdown-body"><h2 id="maximal-likehood-estimate-mle">Maximal Likehood Estimate (MLE)</h2>
<p>Here we change to another topic, the probability field.
We know a lot of prob. distributions, like gassuan, possion, etc.. Now the setting is: you got plenty data samples, and you tell me what is the dist. of these samples thus I can use the dist. to predict other unknow samples.</p>
<p>So, now you have the data set:</p>
<p>$$ D = [ x_1, x_2, x_3, \dots, x_N ]$$</p>
<p>And bunch parameters that we dont know:</p>
<p>$$\theta=[ \theta_1,\theta_2,&hellip;\theta_k ]$$</p>
<p>Since now we have the dataset samples, we assume they all from the same dist sharing the same parameters.</p>
<p>$$
P(D,\theta)=\prod_{i=1}^{N} P(x_i;\theta)
$$</p>
<p>In probability statistics, the joint probability of observed samples is called likelihood, which is usually represented by the symbol and sometimes also called likelihood function.</p>
<p>$$
L(D,\theta)=\prod_{i=1}^{N} P(x_i;\theta)
$$</p>
<p>We want to max the likehood function then we can find these parameters:</p>
<p>$$
\hat{\theta}=argmaxL(D,\theta)=argmax\prod_{i=1}^{N} P(x_i;\theta)
$$</p>
<p>And we want to avoid multiplying multiple p results in the product approaching 0. So, do the log:</p>
<p>$$l(\theta,D)=log(L(D,\theta))$$</p>
<p>$$\hat{\theta}=argmax l(D,\theta)=argmax\sum_{i=1}^{N} log(P(x_i;\theta))$$</p>
<h2 id="mle-in-categorical-distribution">MLE in categorical distribution</h2>
<p>Now, there K classes, categorical distribution:</p>
<p>$$
P(x=c_k;\theta)=\theta_k
$$</p>
<p>Then:</p>
<p>$$P(x;\theta)=\prod_{k=1}^{K} \theta_k^{1(x,x_k)}$$</p>
<p>Also we have to:</p>
<p>$$\sum_i \theta_i=1$$</p>
<p>Thus, recall the MLE we got:</p>
<p>$$L(\theta;D)=\prod_{i=1}^{N} P(x;\theta)=\prod_{i=1}^{N}\prod_{k=1}^{K} \theta_k^{1(x_i,x_k)}$$</p>
<p>$$L(\theta;D)=\prod_{k=1}^{K} \theta_k^{n_k}$$</p>
<p>$n_k$ is the number of class k shown in the D.
Thus, we can get log-likehood function:</p>
<p>$$l(\theta;D)=\sum_{k=1}^{K} n_klog(\theta_k)=\sum_{k=1}^{K} n_klog(P(x=c_k;\theta))$$</p>
<p>Then we can use Lagrange multiplier to find the solution, cuz this is a constraint optimatizion problem:
$$\hat{\theta}=argmax \sum_{k=1}^{K}n_klog(P(x=c_k;\theta))$$</p>
<p>$$La=\sum_{k=1}^{K}n_klog(P(x=c_k;\theta))+\lambda (\sum_{k=1}^{K}\theta_k-1)$$</p>
<p>$$La=\sum_{k=1}^{K}n_klog(\theta_k)+\lambda (\sum_{k=1}^{K}\theta_k-1)$$
Set the partial derivative equal to 0:</p>
<p>$$\frac{\partial La}{\partial \theta _k} =\frac{n_k}{\theta_k} + \lambda =0$$</p>
<p>$$\frac{\partial La}{\partial \lambda}=\theta_1+\theta_2+&hellip;+\theta_K-1=0$$</p>
<p>Thus we got:</p>
<p>$$\Rightarrow \lambda=-N$$
$$\Rightarrow \hat{ \theta_k}=\frac{n_k}{N}$$</p>
<p>Back to likehood function:
$$l(\theta;D)\propto \frac{l(\theta;D)}{N} =\sum_{k=1}^{K} \hat{\theta_k}log(\theta_k)$$
If we multiply it with -1, we can get the cross-entropy:
About the entropy, you can see htttp://asdadada</p>
<p>$$
H(p,q)=-\sum_{j=1} p_j log(q_j)
$$
Here we go:</p>
<p>$$
-l(\theta;D)\propto -\frac{l(\theta;D)}{N} =-\sum_{k=1}^{K} \hat{\theta_k}log(\theta_k)=H(\theta,\hat{\theta})
$$</p>
<p>So, if we wanna maximize the log-likehood function it means we are minimizing the Corss-Entopy between the two distributions. Also it mean decreasing the KL distance between the two distributions. That make sense!</p>
<h2 id="mle-in-gaussian-distribution">MLE in gaussian distribution</h2>
<p>Now, assume the difference  between $X$ and  $\hat {X}$ fit the gaussian distribution $\mathcal{N}(0,\sigma)$ . Thus $X \sim \mathcal{N}(\hat{X},\sigma)$
$$
P(x;\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{}{2\sigma}}
$$
Thus, recall the MLE we got:
$$
L(\theta;D)=\prod_{i=1}^{N} P(x_i;\theta)=\prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(X-\hat{X})^2}{2\sigma^2}}
$$</p>
<p>Thus, we can get log-likehood function:
$$
l(\theta;D)=\sum_{i=1}^{N}log\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{(X-\hat{X})^2}{2\sigma^2}
$$</p>
<p>$$\hat{\theta}=argmax \sum_{k=1}^{K}n_klog(P(x=c_k;\theta))
$$</p>
<p>$$La=\sum_{k=1}^{K}n_klog(P(x=c_k;\theta))+\lambda (\sum_{k=1}^{K}\theta_k-1)
$$</p>
<p>$$La=\sum_{k=1}^{K}n_klog(\theta_k)+\lambda (\sum_{k=1}^{K}\theta_k-1)
$$</p>
</article><section class="article labels"><a class="category" href=/categories/math-in-deep-learning/>Math in Deep Learning</a><a class="tag" href=/tags/math/>Math</a><a class="tag" href=/tags/ml/>ML</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/posts/periodic_solids/"><span class="iconfont icon-article"></span>Periodic Solids</a></p></section><section class="article discussion"><script 
            src="https://utteranc.es/client.js" 
            repo="MetalBlueberry/MetalBlueberry.github.io"
            issue-term="pathname"
            label=""
            theme="github-light"
            crossorigin="anonymous" async>
        </script></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Â©2025 JunfuTan.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank" rel="noopener noreferrer">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank" rel="noopener noreferrer">Notepadium</a>
<a href='https://ipv6-test.com/validate.php?url=referer'
  ><img loading="lazy" src='https://ipv6-test.com/button-ipv6-80x15.png' 
        alt='ipv6 ready' title='ipv6 ready' border='0'
/></a>
</p></div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script
            type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></body>

</html>